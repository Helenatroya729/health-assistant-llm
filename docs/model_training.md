# Дообучение модели

LLaMA 3.1 дообучалась с использованием LoRA-адаптера:

- **Загрузка модели**: 4-битная квантизация для оптимизации памяти (`load_model.py`).
- **Настройка LoRA**: Ранг r=16, целевые модули внимания (`lora_config.py`).
- **Обучение**: 15 эпох, learning_rate=1e-4, с накоплением градиентов (`train_config.py`, `train_model.py`).

Итоговый средний Train Loss составил 0.245, что указывает на успешное обучение модели.